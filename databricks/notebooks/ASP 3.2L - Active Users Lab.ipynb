{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc3284cd-33d7-4327-97e2-b07e7b0784f6"}}},{"cell_type":"markdown","source":["# Active Users Lab\nPlot daily active users and average active users by day of week.\n1. Extract timestamp and date of events\n2. Get daily active users\n3. Get average number of active users by day of week\n4. Sort day of week in correct order"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b026a16-cab2-4ee7-8ec0-0d57ea74df85"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38764b9f-c7d2-4541-9cd6-565e8af94102"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame of user IDs and timestamps of events logged on the BedBricks website."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"509a1a12-3e3c-439d-9a82-e1fdd55a3ad0"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = (spark\n      .read\n      .format(\"delta\")\n      .load(events_path)\n      .select(\"user_id\", col(\"event_timestamp\").alias(\"ts\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e96a9c8-4f53-4389-9fbf-b4e4a16ff188"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Extract timestamp and date of events\n- Convert **`ts`** from microseconds to seconds by dividing by 1 million and cast to timestamp\n- Add **`date`** column by converting **`ts`** to date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2bac13d-b34f-4cff-ac98-08e6dffd1f16"}}},{"cell_type":"code","source":["# TODO\ndatetime_df = (df.FILL_IN\n)\ndisplay(datetime_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4796027-0c46-469c-8f28-b4ca244c7245"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d5b3a9a-0a3f-43b7-a644-9baf74d0af8f"}}},{"cell_type":"code","source":["from pyspark.sql.types import DateType, StringType, StructField, StructType, TimestampType\n\nexpected1a = StructType([StructField(\"user_id\", StringType(), True),\n                         StructField(\"ts\", TimestampType(), True),\n                         StructField(\"date\", DateType(), True)])\n\nresult1a = datetime_df.schema\n\nassert expected1a == result1a, \"datetime_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a7efabf-56ca-4e7e-b6f8-5e9afa3c1369"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\n\nexpected1b = datetime.date(2020, 6, 19)\nresult1b = datetime_df.sort(\"date\").first().date\n\nassert expected1b == result1b, \"datetime_df does not have the expected date values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1648e687-a5ff-407a-9dae-d1354dd88957"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get daily active users\n- Group by date\n- Aggregate approximate count of distinct **`user_id`** and alias to \"active_users\"\n  - Recall built-in function to get **approximate count distinct** (also recall:  approximate count distinct is different than count distinct!)\n- Sort by date\n- Plot as line graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fc87dbd-3d8e-4a9e-a022-be3b5ffa96f3"}}},{"cell_type":"code","source":["# TODO\nactive_users_df = (datetime_df.FILL_IN\n)\ndisplay(active_users_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8313cf90-63b3-4177-b99c-3549254d7257"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd484e99-8c56-4a4e-88ac-e729a8c9cff7"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\nexpected2a = StructType([StructField(\"date\", DateType(), True),\n                         StructField(\"active_users\", LongType(), False)])\n\nresult2a = active_users_df.schema\n\nassert expected2a == result2a, \"active_users_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2456ab7a-1364-45ca-b49d-50c3129df971"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected2b = [(datetime.date(2020, 6, 19), 251573), (datetime.date(2020, 6, 20), 357215), (datetime.date(2020, 6, 21), 305055), (datetime.date(2020, 6, 22), 239094), (datetime.date(2020, 6, 23), 243117)]\n\nresult2b = [(row.date, row.active_users) for row in active_users_df.orderBy(\"date\").take(5)]\n\nassert expected2b == result2b, \"active_users_df does not have the expected values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74ed3dff-1372-4612-b2b1-fa7fbafc335b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Get average number of active users by day of week\n- Add **`day`** column by extracting day of week from **`date`** using a datetime pattern string - the expected output here will be a day name, not a number (e.g. **`Mon`**, not **`1`**)\n- Group by **`day`**\n- Aggregate average of **`active_users`** and alias to \"avg_users\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1fcd1a8-e3ef-4f97-8755-23f17d1bc46c"}}},{"cell_type":"code","source":["# TODO\nactive_dow_df = (active_users_df.FILL_IN\n)\ndisplay(active_dow_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d16a962-525a-4c58-ab7e-5fe37647c222"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2339f769-ff42-4d53-98e6-161a10143c4e"}}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n\nexpected3a = StructType([StructField(\"day\", StringType(), True),\n                         StructField(\"avg_users\", DoubleType(), True)])\n\nresult3a = active_dow_df.schema\n\nassert expected3a == result3a, \"active_dow_df does not have the expected schema\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5cdd0d5-d29c-4b62-b685-1d45de53bef6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected3b = [(\"Fri\", 247180.66666666666), (\"Mon\", 238195.5), (\"Sat\", 278482.0), (\"Sun\", 282905.5), (\"Thu\", 264620.0), (\"Tue\", 260942.5), (\"Wed\", 227214.0)]\n\nresult3b = [(row.day, row.avg_users) for row in active_dow_df.sort(\"day\").collect()]\n\nassert expected3b == result3b, \"active_dow_df does not have the expected values\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f782d37-4158-448d-8edc-27437bf4d9c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f40f2db0-6b4b-471a-90f8-0d079e945ac6"}}},{"cell_type":"code","source":["classroom_cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ec17fc-ca47-47e5-bca4-68c9e3529fd3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bc0a8a5-857c-4189-8030-31630ceb1558"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 3.2L - Active Users Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3745653733761568}},"nbformat":4,"nbformat_minor":0}
