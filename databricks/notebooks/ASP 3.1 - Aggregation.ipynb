{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a09361d-8b34-4f59-b03e-56eb9f1c9678"}}},{"cell_type":"markdown","source":["# Aggregation\n\n##### Objectives\n1. Group data by specified columns\n1. Apply grouped data methods to aggregate data\n1. Apply built-in functions to aggregate data\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: **`groupBy`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html#pyspark.sql.GroupedData\" target=\"_blank\" target=\"_blank\">Grouped Data</a>: **`agg`**, **`avg`**, **`count`**, **`max`**, **`sum`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>: **`approx_count_distinct`**, **`avg`**, **`sum`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce566542-00b9-4a5b-ade2-fcb3ef09c4aa"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"802b04e3-8bf8-48b0-a3c8-8340e690ac12"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use the BedBricks events dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b12dbd4e-1b5b-4281-ae41-12d17590a40f"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(events_path)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2605ca87-0347-446a-a0bc-b5aa04a7d32b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Grouping data\n\n<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"975061e9-8e78-4bf3-9d23-174c8f906a56"}}},{"cell_type":"markdown","source":["### groupBy\nUse the DataFrame **`groupBy`** method to create a grouped data object. \n\nThis grouped data object is called **`RelationalGroupedDataset`** in Scala and **`GroupedData`** in Python."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d152f0a6-737a-40dc-9867-a5dd90225465"}}},{"cell_type":"code","source":["df.groupBy(\"event_name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"768c60d7-3c22-45c5-a46e-4f7470f23886"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.groupBy(\"geo.state\", \"geo.city\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1c5fb0f-0d87-4b75-90af-8d826777ee9b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Grouped data methods\nVarious aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html\" target=\"_blank\">GroupedData</a> object.\n\n\n| Method | Description |\n| --- | --- |\n| agg | Compute aggregates by specifying a series of aggregate columns |\n| avg | Compute the mean value for each numeric columns for each group |\n| count | Count the number of rows for each group |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| min | Compute the min value for each numeric column for each group |\n| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n| sum | Compute the sum for each numeric columns for each group |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5bbfa61-5444-4ed6-904d-b3d4bfe4f858"}}},{"cell_type":"code","source":["event_counts_df = df.groupBy(\"event_name\").count()\ndisplay(event_counts_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90f9d81c-63f8-4cf4-ab1a-d1d28577a12e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here, we're getting the average purchase revenue for each."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"850f56ce-0868-4bf3-8541-cf55a5583068"}}},{"cell_type":"code","source":["avg_state_purchases_df = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\ndisplay(avg_state_purchases_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a78323d-a37b-4422-9a0a-4c2f832ff1b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And here the total quantity and sum of the purchase revenue for each combination of state and city."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10e14f9d-bde4-4035-9a3b-9fb0c51acdfe"}}},{"cell_type":"code","source":["city_purchase_quantities_df = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\ndisplay(city_purchase_quantities_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2a51c72-9b28-4b21-9c89-bf5d159129b8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Built-In Functions\nIn addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module.\n\nIn Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_blank\">**`org.apache.spark.sql.functions`**</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">**`pyspark.sql.functions`**</a> in Python. Functions from this module must be imported into your code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97c4a47f-81c4-4df1-b279-212505570552"}}},{"cell_type":"markdown","source":["### Aggregate Functions\n\nHere are some of the built-in functions available for aggregation.\n\n| Method | Description |\n| --- | --- |\n| approx_count_distinct | Returns the approximate number of distinct items in a group |\n| avg | Returns the average of the values in a group |\n| collect_list | Returns a list of objects with duplicates |\n| corr | Returns the Pearson Correlation Coefficient for two columns |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| stddev_samp | Returns the sample standard deviation of the expression in a group |\n| sumDistinct | Returns the sum of distinct values in the expression |\n| var_pop | Returns the population variance of the values in a group |\n\nUse the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">**`agg`**</a> to apply built-in aggregate functions\n\nThis allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">**`alias`**</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09012547-78e9-4538-9d7b-4e767006b41a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n\nstate_purchases_df = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\ndisplay(state_purchases_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eea3cfc0-5b49-4c67-bf5d-2f02de8c8cdf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Apply multiple aggregate functions on grouped data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64b5d4f0-7421-4aba-af6a-875bda601150"}}},{"cell_type":"code","source":["from pyspark.sql.functions import avg, approx_count_distinct\n\nstate_aggregates_df = (df\n                       .groupBy(\"geo.state\")\n                       .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n                            approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n                      )\n\ndisplay(state_aggregates_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4964acce-faf5-439e-ba62-439fe9f0ba01"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Math Functions\nHere are some of the built-in functions for math operations.\n\n| Method | Description |\n| --- | --- |\n| ceil | Computes the ceiling of the given column. |\n| cos | Computes the cosine of the given value. |\n| log | Computes the natural logarithm of the given value. |\n| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n| sqrt | Computes the square root of the specified float value. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ceef6290-b4ce-479a-83b1-586dfac7be97"}}},{"cell_type":"code","source":["from pyspark.sql.functions import cos, sqrt\n\ndisplay(spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n        .withColumn(\"sqrt\", sqrt(\"id\"))\n        .withColumn(\"cos\", cos(\"id\"))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37bb17ff-2ce7-4e20-82e3-9099743455a2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a63f91e-bcdd-4cff-8f87-b8308bc2eb3b"}}},{"cell_type":"code","source":["classroom_cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74749274-b1e5-4434-9647-51c99887d88c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d16ee09-aa09-4477-9bfa-82687055505a"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 3.1 - Aggregation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3745653733761395}},"nbformat":4,"nbformat_minor":0}
