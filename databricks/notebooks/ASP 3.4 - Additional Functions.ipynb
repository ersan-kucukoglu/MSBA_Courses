{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1a97d19-f9b5-4367-9477-bfdebca723ef"}}},{"cell_type":"markdown","source":["# Additional Functions\n\n##### Objectives\n1. Apply built-in functions to generate data for new columns\n1. Apply DataFrame NA functions to handle null values\n1. Join DataFrames\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameNaFunctions.html\" target=\"_blank\">DataFrameNaFunctions</a>: **`fill`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>:\n  - Aggregate: **`collect_set`**\n  - Collection: **`explode`**\n  - Non-aggregate and miscellaneous: **`col`**, **`lit`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"850e2bf2-ff8d-4e7a-8bb8-22107cc9bf9d"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23b6ec92-f6e9-4d78-8dfe-a2f417b17a0e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffd8c495-b2b1-4fbf-b584-944cf9277019"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### DataFrameNaFunctions\n<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameNaFunctions.html\" target=\"_blank\">DataFrameNaFunctions</a> is a DataFrame submodule with methods for handling null values. Obtain an instance of DataFrameNaFunctions by accessing the **`na`** attribute of a DataFrame.\n\n| Method | Description |\n| --- | --- |\n| drop | Returns a new DataFrame omitting rows with any, all, or a specified number of null values, considering an optional subset of columns |\n| fill | Replace null values with the specified value for an optional subset of columns |\n| replace | Returns a new DataFrame replacing a value with another value, considering an optional subset of columns |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bc00533-5de7-4425-aee1-bbb651c75c0d"}}},{"cell_type":"code","source":["sales_df = spark.read.format(\"delta\").load(sales_path)\ndisplay(sales_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8485d798-8dfe-4031-9465-81966d8e05ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's say we need to remove the email addresses from our dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4dc7751-36c5-4b24-92bc-e4f651f467e8"}}},{"cell_type":"code","source":["no_pii_df = sales_df.drop(\"email\")\n\ndisplay(no_pii_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"146db0cd-05ac-41be-842e-7fad031513c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Non-aggregate and Miscellaneous Functions\nHere are a few additional non-aggregate and miscellaneous built-in functions.\n\n| Method | Description |\n| --- | --- |\n| col / column | Returns a Column based on the given column name. |\n| lit | Creates a Column of literal value |\n| isnull | Return true iff the column is null |\n| rand | Generate a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0) |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f105df9-0374-41a3-af75-ecf192f7dee2"}}},{"cell_type":"markdown","source":["We could select a particular column using the **`col`** function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e987a524-97c7-4099-9919-c29a6e33c7b3"}}},{"cell_type":"code","source":["gmail_accounts = sales_df.filter(col(\"email\").contains(\"gmail\"))\n\ndisplay(gmail_accounts)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15ef50ca-0712-4e41-aae7-eaad56370e00"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Joining DataFrames\nThe DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join\" target=\"_blank\">**`join`**</a> method joins two DataFrames based on a given join expression. \n\nSeveral different types of joins are supported:\n\nInner join based on equal values of a shared column called \"name\" (i.e., an equi join)<br/>\n**`df1.join(df2, \"name\")`**\n\nInner join based on equal values of the shared columns called \"name\" and \"age\"<br/>\n**`df1.join(df2, [\"name\", \"age\"])`**\n\nFull outer join based on equal values of a shared column called \"name\"<br/>\n**`df1.join(df2, \"name\", \"outer\")`**\n\nLeft outer join based on an explicit column expression<br/>\n**`df1.join(df2, df1[\"customer_name\"] == df2[\"account_name\"], \"left_outer\")`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b4f5837-01d6-4203-9ec4-1596101da8db"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce4c5f7b-1a34-48c7-a548-64fc9eb09d7b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 3.4 - Additional Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3745653733761619}},"nbformat":4,"nbformat_minor":0}
