{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34f46de6-d506-4f5c-b867-042619a1bebd"}}},{"cell_type":"markdown","source":["# Reader & Writer\n##### Objectives\n1. Read from CSV files\n1. Read from JSON files\n1. Write DataFrame to files\n1. Write DataFrame to tables\n1. Write DataFrame to a Delta table\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>: **`csv`**, **`json`**, **`option`**, **`schema`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>: **`mode`**, **`option`**, **`parquet`**, **`format`**, **`saveAsTable`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType\" target=\"_blank\">StructType</a>: **`toDDL`**\n\n##### Spark Types\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\" target=\"_blank\">Types</a>: **`ArrayType`**, **`DoubleType`**, **`IntegerType`**, **`LongType`**, **`StringType`**, **`StructType`**, **`StructField`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffe4a040-ecae-4629-9c35-f31f126b0ece"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecd4cb51-2242-425a-96f9-32524aca5194"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrameReader\nInterface used to load a DataFrame from external storage systems\n\n**`spark.read.parquet(\"path/to/files\")`**\n\nDataFrameReader is accessible through the SparkSession attribute **`read`**. This class includes methods to load DataFrames from different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73fdc05e-13bb-4d64-b770-93ca26ba78e0"}}},{"cell_type":"markdown","source":["### Read from CSV files\nRead from CSV with the DataFrameReader's **`csv`** method and the following options:\n\nTab separator, use first line as header, infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63860f96-638b-472b-9ddf-c517a110f038"}}},{"cell_type":"code","source":["users_csv_path = f\"{datasets_dir}/users/users-500k.csv\"\n\nusers_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .option(\"inferSchema\", True)\n           .csv(users_csv_path)\n          )\n\nusers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"475060ee-3c49-4c36-84c0-fbc7bd683429"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark's Python API also allows you to specify the DataFrameReader options as parameters to the **`csv`** method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"047d0458-8574-465b-8388-9b02326a277a"}}},{"cell_type":"code","source":["users_df = (spark\n           .read\n           .csv(users_csv_path, sep=\"\\t\", header=True, inferSchema=True)\n          )\n\nusers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33c2abef-5fae-422c-b4f8-9ab07896802a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Manually define the schema by creating a **`StructType`** with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e3bcaf1-01f0-4dd4-959e-5942b8e6dfe5"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType, StringType, StructType, StructField\n\nuser_defined_schema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"email\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96a7f56a-4a26-4d30-9228-5d25298653f6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read from CSV using this user-defined schema instead of inferring the schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b4bbad1-dd18-48e6-9103-cfcd5510b036"}}},{"cell_type":"code","source":["users_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(user_defined_schema)\n           .csv(users_csv_path)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbf914b0-57ab-4e63-a69c-b0ee424049b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternatively, define the schema using <a href=\"https://en.wikipedia.org/wiki/Data_definition_language\" target=\"_blank\">data definition language (DDL)</a> syntax."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ace6648a-3c87-44a9-a4e8-9b2d7519d1d2"}}},{"cell_type":"code","source":["ddl_schema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusers_df = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(ddl_schema)\n           .csv(users_csv_path)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"861bd907-5090-4fe8-b983-7bbebaf2901a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read from JSON files\n\nRead from JSON with DataFrameReader's **`json`** method and the infer schema option"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59723696-fe9d-4e4b-9596-683aa3b380f4"}}},{"cell_type":"code","source":["events_json_path = f\"{datasets_dir}/events/events-500k.json\"\n\nevents_df = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .json(events_json_path)\n           )\n\nevents_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcfd9b37-8387-4e58-b17a-e3b16712bfad"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read data faster by creating a **`StructType`** with the schema names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e412e35d-8f49-4061-bcde-91e821aa9c38"}}},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuser_defined_schema = StructType([\n    StructField(\"device\", StringType(), True),\n    StructField(\"ecommerce\", StructType([\n        StructField(\"purchaseRevenue\", DoubleType(), True),\n        StructField(\"total_item_quantity\", LongType(), True),\n        StructField(\"unique_items\", LongType(), True)\n    ]), True),\n    StructField(\"event_name\", StringType(), True),\n    StructField(\"event_previous_timestamp\", LongType(), True),\n    StructField(\"event_timestamp\", LongType(), True),\n    StructField(\"geo\", StructType([\n        StructField(\"city\", StringType(), True),\n        StructField(\"state\", StringType(), True)\n    ]), True),\n    StructField(\"items\", ArrayType(\n        StructType([\n            StructField(\"coupon\", StringType(), True),\n            StructField(\"item_id\", StringType(), True),\n            StructField(\"item_name\", StringType(), True),\n            StructField(\"item_revenue_in_usd\", DoubleType(), True),\n            StructField(\"price_in_usd\", DoubleType(), True),\n            StructField(\"quantity\", LongType(), True)\n        ])\n    ), True),\n    StructField(\"traffic_source\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"user_id\", StringType(), True)\n])\n\nevents_df = (spark\n            .read\n            .schema(user_defined_schema)\n            .json(events_json_path)\n           )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a68b780e-d00d-4390-99a8-0e3819bf571c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can use the **`StructType`** Scala method **`toDDL`** to have a DDL-formatted string created for you.\n\nThis is convenient when you need to get the DDL-formated string for ingesting CSV and JSON but you don't want to hand craft it or the **`StructType`** variant of the schema.\n\nHowever, this functionality is not available in Python but the power of the notebooks allows us to use both languages."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29d51823-79fc-4fc7-b2da-fd8ec17efd03"}}},{"cell_type":"code","source":["# Step 1 - use this trick to transfer a value (the dataset path) between Python and Scala using the shared spark-config\nspark.conf.set(\"com.whatever.your_scope.events_path\", events_json_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8e97f4c-69d6-4c2a-90cc-e91040155963"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In a Python notebook like this one, create a Scala cell to injest the data and produce the DDL formatted schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08c9f435-86d4-4c4a-900f-9d96fe648dc7"}}},{"cell_type":"code","source":["%scala\n// Step 2 - pull the value from the config (or copy & paste it)\nval eventsJsonPath = spark.conf.get(\"com.whatever.your_scope.events_path\")\n\n// Step 3 - Read in the JSON, but let it infer the schema\nval eventsSchema = spark.read\n                        .option(\"inferSchema\", true)\n                        .json(eventsJsonPath)\n                        .schema.toDDL\n\n// Step 4 - print the schema, select it, and copy it.\nprintln(\"=\"*80)\nprintln(eventsSchema)\nprintln(\"=\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b752853-93d7-429f-a08a-2ec9e6317d1a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Step 5 - paste the schema from above and assign it to a variable as seen here\nevents_schema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n\n# Step 6 - Read in the JSON data using our new DDL formatted string\nevents_df = (spark.read\n                 .schema(events_schema)\n                 .json(events_json_path))\n\ndisplay(events_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79b05db6-f061-4095-b466-de0e6386624c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This is a great \"trick\" for producing a schema for a net-new dataset and for accelerating development.\n\nWhen you are done (e.g. for Step #7), make sure to delete your temporary code.\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> WARNING: **Do not use this trick in production**</br>\nthe inference of a schema can be REALLY slow as it<br/>\nforces a full read of the source dataset to infer the schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1055a2b9-e579-4615-bb68-34b532270540"}}},{"cell_type":"markdown","source":["## DataFrameWriter\nInterface used to write a DataFrame to external storage systems\n\n<strong><code>\n(df  \n&nbsp;  .write                         \n&nbsp;  .option(\"compression\", \"snappy\")  \n&nbsp;  .mode(\"overwrite\")      \n&nbsp;  .parquet(output_dir)       \n)\n</code></strong>\n\nDataFrameWriter is accessible through the SparkSession attribute **`write`**. This class includes methods to write DataFrames to different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"908dacd6-f5de-4ab6-8bdd-24aab0c54302"}}},{"cell_type":"markdown","source":["### Write DataFrames to files\n\nWrite **`users_df`** to parquet with DataFrameWriter's **`parquet`** method and the following configurations:\n\nSnappy compression, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4dc360a-d38c-40b8-96a7-e23848b06039"}}},{"cell_type":"code","source":["users_output_dir = working_dir + \"/users.parquet\"\n\n(users_df\n .write\n .option(\"compression\", \"snappy\")\n .mode(\"overwrite\")\n .parquet(users_output_dir)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d39beba-5b32-40c5-9393-aa0ca3492ae3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(\n    dbutils.fs.ls(users_output_dir)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"001591ba-5e23-44b6-a843-3e71cf86ec99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the **`parquet`** method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76a75f43-041f-48c2-ba9b-8ef88d2b8b4b"}}},{"cell_type":"code","source":["(users_df\n .write\n .parquet(users_output_dir, compression=\"snappy\", mode=\"overwrite\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bd6f35b-c2c2-4664-8b87-fac87a039312"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write DataFrames to tables\n\nWrite **`events_df`** to a table using the DataFrameWriter method **`saveAsTable`**\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This creates a global table, unlike the local view created by the DataFrame method **`createOrReplaceTempView`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adb25101-e53f-4066-b468-55c74b82fb06"}}},{"cell_type":"code","source":["events_df.write.mode(\"overwrite\").saveAsTable(\"events\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"269b31eb-b396-46f3-ab65-5d8f93375a03"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This table was saved in the database created for you in classroom setup. See database name printed below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb2f9696-97e6-4432-85a8-44a6000072d0"}}},{"cell_type":"code","source":["print(database_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30ef2a2f-7cb5-4fb3-8a6a-c3ebde6d0b8a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Delta Lake\n\nIn almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. \n\n<a href=\"https://delta.io/\" target=\"_blank\">Delta Lake</a> is an open source technology designed to work with Spark to bring reliability to data lakes.\n\n![delta](https://files.training.databricks.com/images/aspwd/delta_storage_layer.png)\n\n#### Delta Lake's Key Features\n- ACID transactions\n- Scalable metadata handling\n- Unified streaming and batch processing\n- Time travel (data versioning)\n- Schema enforcement and evolution\n- Audit history\n- Parquet format\n- Compatible with Apache Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a25d521-83d1-4a9c-8364-6247aec84983"}}},{"cell_type":"markdown","source":["### Write Results to a Delta Table\n\nWrite **`events_df`** with the DataFrameWriter's **`save`** method and the following configurations: Delta format & overwrite mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0d001cb-dd5b-4f27-9683-7657ab5e6e9b"}}},{"cell_type":"code","source":["events_output_path = working_dir + \"/delta/events\"\n\n(events_df\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(events_output_path)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8edae065-3d0c-4503-8c4a-b2259c98b72b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b08365df-9314-4b8c-b0d1-8f6725ce36bf"}}},{"cell_type":"code","source":["classroom_cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"852b0b70-8ccd-46dd-9f73-02b8405fb139"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"424f8be2-90f5-40e4-936e-0b446367983c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.2 - Reader & Writer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3745653733761287}},"nbformat":4,"nbformat_minor":0}
